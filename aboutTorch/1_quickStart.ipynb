{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dataset and dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "# torch.utils.data.datasets \n",
    "from torchvision import datasets # torchvision, torchaudio 등 도메인특화 datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "# torchvision.transforms는 데이터 전처리 package로 inference를 위한 현실데이터를 변환하는데도 사용된다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0118, 0.0039, 0.0000, 0.0000, 0.0275,\n",
      "          0.0000, 0.1451, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0039, 0.0078, 0.0000,\n",
      "          0.1059, 0.3294, 0.0431, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.4667, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0039, 0.0000, 0.0000,\n",
      "          0.3451, 0.5608, 0.4314, 0.0000, 0.0000, 0.0000, 0.0000, 0.0863,\n",
      "          0.3647, 0.4157, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0157, 0.0000, 0.2078,\n",
      "          0.5059, 0.4706, 0.5765, 0.6863, 0.6157, 0.6510, 0.5294, 0.6039,\n",
      "          0.6588, 0.5490, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0078, 0.0000, 0.0431, 0.5373,\n",
      "          0.5098, 0.5020, 0.6275, 0.6902, 0.6235, 0.6549, 0.6980, 0.5843,\n",
      "          0.5922, 0.5647, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0039, 0.0000,\n",
      "          0.0078, 0.0039, 0.0000, 0.0118, 0.0000, 0.0000, 0.4510, 0.4471,\n",
      "          0.4157, 0.5373, 0.6588, 0.6000, 0.6118, 0.6471, 0.6549, 0.5608,\n",
      "          0.6157, 0.6196, 0.0431, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0039, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0118, 0.0000, 0.0000, 0.3490, 0.5451, 0.3529,\n",
      "          0.3686, 0.6000, 0.5843, 0.5137, 0.5922, 0.6627, 0.6745, 0.5608,\n",
      "          0.6235, 0.6627, 0.1882, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0078, 0.0157,\n",
      "          0.0039, 0.0000, 0.0000, 0.0000, 0.3843, 0.5333, 0.4314, 0.4275,\n",
      "          0.4314, 0.6353, 0.5294, 0.5647, 0.5843, 0.6235, 0.6549, 0.5647,\n",
      "          0.6196, 0.6627, 0.4667, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0078, 0.0078, 0.0039, 0.0078, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.1020, 0.4235, 0.4588, 0.3882, 0.4353, 0.4588,\n",
      "          0.5333, 0.6118, 0.5255, 0.6039, 0.6039, 0.6118, 0.6275, 0.5529,\n",
      "          0.5765, 0.6118, 0.6980, 0.0000],\n",
      "         [0.0118, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0824,\n",
      "          0.2078, 0.3608, 0.4588, 0.4353, 0.4039, 0.4510, 0.5059, 0.5255,\n",
      "          0.5608, 0.6039, 0.6471, 0.6667, 0.6039, 0.5922, 0.6039, 0.5608,\n",
      "          0.5412, 0.5882, 0.6471, 0.1686],\n",
      "         [0.0000, 0.0000, 0.0902, 0.2118, 0.2549, 0.2980, 0.3333, 0.4627,\n",
      "          0.5020, 0.4824, 0.4353, 0.4431, 0.4627, 0.4980, 0.4902, 0.5451,\n",
      "          0.5216, 0.5333, 0.6275, 0.5490, 0.6078, 0.6314, 0.5647, 0.6078,\n",
      "          0.6745, 0.6314, 0.7412, 0.2431],\n",
      "         [0.0000, 0.2667, 0.3686, 0.3529, 0.4353, 0.4471, 0.4353, 0.4471,\n",
      "          0.4510, 0.4980, 0.5294, 0.5333, 0.5608, 0.4941, 0.4980, 0.5922,\n",
      "          0.6039, 0.5608, 0.5804, 0.4902, 0.6353, 0.6353, 0.5647, 0.5412,\n",
      "          0.6000, 0.6353, 0.7686, 0.2275],\n",
      "         [0.2745, 0.6627, 0.5059, 0.4078, 0.3843, 0.3922, 0.3686, 0.3804,\n",
      "          0.3843, 0.4000, 0.4235, 0.4157, 0.4667, 0.4706, 0.5059, 0.5843,\n",
      "          0.6118, 0.6549, 0.7451, 0.7451, 0.7686, 0.7765, 0.7765, 0.7333,\n",
      "          0.7725, 0.7412, 0.7216, 0.1412],\n",
      "         [0.0627, 0.4941, 0.6706, 0.7373, 0.7373, 0.7216, 0.6706, 0.6000,\n",
      "          0.5294, 0.4706, 0.4941, 0.4980, 0.5725, 0.7255, 0.7647, 0.8196,\n",
      "          0.8157, 1.0000, 0.8196, 0.6941, 0.9608, 0.9882, 0.9843, 0.9843,\n",
      "          0.9686, 0.8627, 0.8078, 0.1922],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0471, 0.2627, 0.4157, 0.6431, 0.7255,\n",
      "          0.7804, 0.8235, 0.8275, 0.8235, 0.8157, 0.7451, 0.5882, 0.3216,\n",
      "          0.0314, 0.0000, 0.0000, 0.0000, 0.6980, 0.8157, 0.7373, 0.6863,\n",
      "          0.6353, 0.6196, 0.5922, 0.0431],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000]]]), 9)\n",
      "tensor data's feature size is:torch.Size([64, 1, 28, 28])\n",
      "label tensor size is torch.Size([64]), dtype is torch.int64\n"
     ]
    }
   ],
   "source": [
    "# sample과 label로 구성되어 있는 데이터셋 로드\n",
    "# transform을 통해 sample 변환, target_transform을 통해 label변환\n",
    "# ToTensor로 변환시 sample(img) 차원은 [1,28,28]\n",
    "train_data = datasets.FashionMNIST(\n",
    "    root ='./',\n",
    "    train = True,\n",
    "    download = True,\n",
    "    transform = ToTensor()\n",
    ")\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root ='./',\n",
    "    train = False,\n",
    "    download= True,\n",
    "    transform = ToTensor()\n",
    ")\n",
    "print(test_data[0])\n",
    "# 64개씩 묶음\n",
    "batch_size = 64\n",
    "# dataloader는 feature와 label로 이루어진 데이터를 64개씩 묶어 iterable한 객체로 반환\n",
    "# shuffling sampling 등의 기능도 지원\n",
    "train_loader = DataLoader(train_data, batch_size = batch_size)\n",
    "test_loader = DataLoader(test_data, batch_size = batch_size)\n",
    "\n",
    "for data,label in test_loader:\n",
    "    print(f\"tensor data's feature size is:{data.shape}\")\n",
    "    print(f\"label tensor size is {label.shape}, dtype is {label.dtype}\")\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Generate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tips\n",
    "> layer의 선형변환은 입력차원을 축소 또는 확장 -> 특징의 밀도를 높이거나 낮춘다 (연산량을 줄이거나 특징을 풍부하게 한다)\n",
    "\n",
    "\n",
    "> 입력 데이터의 가장 낮은 차원은 batch size값을 가진다. ex) rgb 28*28 batch 3 -> [3,3,28,28]\n",
    "\n",
    "\n",
    "> flatten을 하여도 dim = 0, 즉 batch 차원은 유지된다. ex) [3,3,28,28] -> [3,784]\n",
    "\n",
    "> gradient는 매개변수에 대한 손실함수의 변화도이다 --> 이에 따라 매개변수가 조정된다.\n",
    "\n",
    "\n",
    "> 비선형 활성화(activation)는 모델의 입력과 출력 사이에 복잡한 관계(mapping)를 만든다. 비선형 활성화는 선형 변환 후에 비선형성(nonlinearity) 을 도입하고, 신경망이 다양한 현상을 학습할 수 있도록 돕는다.\n",
    "\n",
    "\n",
    "> softmax함수의 인자인 dim 은 scale이 조정될 출력벡터 값의 합을 의미한다. ex) nn.Softmax(dim = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device:cuda\n",
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"device:{device}\")\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    # init에서 신경망 구성요소 정의\n",
    "    def __init__(self):\n",
    "        # super().__init__()\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        # sequential은 출력벡터를 반환한다\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28,512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512,512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512,10)\n",
    "        )\n",
    "    # forward 에서 신경망에서 데이터를 처리하는 과정을 지정\n",
    "    def forward(self,x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "model = NeuralNetwork()\n",
    "model.to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss function & Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000\n",
      "938\n"
     ]
    }
   ],
   "source": [
    "loss_f = nn.CrossEntropyLoss()\n",
    "criterion = torch.optim.SGD(model.parameters(), lr = 1e-3)\n",
    "print(len(train_loader.dataset))\n",
    "print(len(train_loader))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train & eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "def train(dataloader, model, loss_f, criterion):\n",
    "    data_size = len(dataloader.dataset)\n",
    "    for batch_number, (data, label) in enumerate(dataloader):\n",
    "        data , label = data.to(device), label.to(device)\n",
    "        prediction = model(data)\n",
    "        loss = loss_f(prediction, label)\n",
    "\n",
    "        criterion.zero_grad()   # 초기 gradient를 0으로 만들어준다 (갱신에 영향이 없게)\n",
    "        loss.backward() # 역전파하며 층별 gradient를 계산\n",
    "        criterion.step()    # gradient를 가중치에 적용하여 갱신 (gradient descent)\n",
    "\n",
    "        if batch_number % 100 == 0:\n",
    "            loss, current = loss.item(), (batch_number+1) * len(data) #item()은 행렬의 원소를 추출 , ex) 100 * 64\n",
    "            print(f\"loss:{loss:.1f}, at data number:{current}/{data_size}\")\n",
    "\n",
    "def test(dataloader, model, loss_f):\n",
    "    data_size = len(dataloader.dataset)\n",
    "    batch_number = len(dataloader)\n",
    "    # model 평가모드\n",
    "    model.eval()\n",
    "    loss, correct = 0, 0\n",
    "    # gradient 변화없이\n",
    "    with torch.no_grad():\n",
    "        for data, label in dataloader:\n",
    "            data , label = data.to(device), label.to(device)\n",
    "            prediction = model(data)\n",
    "\n",
    "            loss += loss_f(prediction, label).item()\n",
    "            correct += (prediction.argmax(1) == label).type(torch.float).sum().item()\n",
    "    loss /= batch_number\n",
    "    acc = (correct/data_size)*100\n",
    "    print(f\"test loss avg:{loss:>0.1f} test acc:{acc:.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1epoch________________________\n",
      "loss:1.2, at data number:64/60000\n",
      "loss:1.2, at data number:6464/60000\n",
      "loss:1.0, at data number:12864/60000\n",
      "loss:1.1, at data number:19264/60000\n",
      "loss:1.0, at data number:25664/60000\n",
      "loss:1.0, at data number:32064/60000\n",
      "loss:1.1, at data number:38464/60000\n",
      "loss:1.0, at data number:44864/60000\n",
      "loss:1.0, at data number:51264/60000\n",
      "loss:1.0, at data number:57664/60000\n",
      "test loss avg:1.0 test acc:65.8\n",
      "2epoch________________________\n",
      "loss:1.1, at data number:64/60000\n",
      "loss:1.1, at data number:6464/60000\n",
      "loss:0.9, at data number:12864/60000\n",
      "loss:1.0, at data number:19264/60000\n",
      "loss:0.9, at data number:25664/60000\n",
      "loss:0.9, at data number:32064/60000\n",
      "loss:1.0, at data number:38464/60000\n",
      "loss:0.9, at data number:44864/60000\n",
      "loss:1.0, at data number:51264/60000\n",
      "loss:0.9, at data number:57664/60000\n",
      "test loss avg:0.9 test acc:67.0\n",
      "3epoch________________________\n",
      "loss:1.0, at data number:64/60000\n",
      "loss:1.0, at data number:6464/60000\n",
      "loss:0.8, at data number:12864/60000\n",
      "loss:1.0, at data number:19264/60000\n",
      "loss:0.9, at data number:25664/60000\n",
      "loss:0.9, at data number:32064/60000\n",
      "loss:0.9, at data number:38464/60000\n",
      "loss:0.9, at data number:44864/60000\n",
      "loss:0.9, at data number:51264/60000\n",
      "loss:0.9, at data number:57664/60000\n",
      "test loss avg:0.9 test acc:68.3\n",
      "4epoch________________________\n",
      "loss:0.9, at data number:64/60000\n",
      "loss:1.0, at data number:6464/60000\n",
      "loss:0.7, at data number:12864/60000\n",
      "loss:0.9, at data number:19264/60000\n",
      "loss:0.8, at data number:25664/60000\n",
      "loss:0.8, at data number:32064/60000\n",
      "loss:0.9, at data number:38464/60000\n",
      "loss:0.8, at data number:44864/60000\n",
      "loss:0.8, at data number:51264/60000\n",
      "loss:0.8, at data number:57664/60000\n",
      "test loss avg:0.8 test acc:69.6\n",
      "5epoch________________________\n",
      "loss:0.8, at data number:64/60000\n",
      "loss:0.9, at data number:6464/60000\n",
      "loss:0.7, at data number:12864/60000\n",
      "loss:0.9, at data number:19264/60000\n",
      "loss:0.8, at data number:25664/60000\n",
      "loss:0.8, at data number:32064/60000\n",
      "loss:0.9, at data number:38464/60000\n",
      "loss:0.8, at data number:44864/60000\n",
      "loss:0.8, at data number:51264/60000\n",
      "loss:0.8, at data number:57664/60000\n",
      "test loss avg:0.8 test acc:71.1\n"
     ]
    }
   ],
   "source": [
    "epochs = 5\n",
    "for i in range(epochs):\n",
    "    print(f\"{i+1}epoch________________________\")\n",
    "    train(train_loader,model, loss_f, criterion)\n",
    "    test(test_loader, model, loss_f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model save & load to Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model saved succedssfully\n"
     ]
    }
   ],
   "source": [
    "# model의 state_dict(상태 dictionary)를 직렬화하여 저장 (매개변수 등이 포함)\n",
    "# model구조를 선언한 후 상태 dict를 load\n",
    "torch.save(model.state_dict(),\"torchModel.pth\")\n",
    "print(f\"model saved succedssfully\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model successfully loaded\n",
      "infered data:Ankle boot, actual data:Ankle boot\n"
     ]
    }
   ],
   "source": [
    "model = NeuralNetwork()\n",
    "model.load_state_dict(torch.load(\"torchModel.pth\"))\n",
    "print(f\"model successfully loaded\")\n",
    "\n",
    "classes = [\n",
    "    \"T-shirt/top\",\n",
    "    \"Trouser\",\n",
    "    \"Pullover\",\n",
    "    \"Dress\",\n",
    "    \"Coat\",\n",
    "    \"Sandal\",\n",
    "    \"Shirt\",\n",
    "    \"Sneaker\",\n",
    "    \"Bag\",\n",
    "    \"Ankle boot\",\n",
    "]\n",
    "\n",
    "model.eval() # 평가모드\n",
    "data, label = test_data[0][0], test_data[0][1]\n",
    "with torch.no_grad():\n",
    "    infer = model(data)\n",
    "    # infer는 [batch_size, logits] 차원의 데이터가 된다\n",
    "    infered, actual = classes[infer[0].argmax(0)],classes[label]\n",
    "    print(f\"infered data:{infered}, actual data:{actual}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference for real-world data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torchvision.transforms as transforms\n",
    "# from PIL import Image\n",
    "# # Define transformations for real-world data\n",
    "# transform = transforms.Compose([\n",
    "#     transforms.Resize((224, 224)),  # Resize to the input size of your model\n",
    "#     transforms.ToTensor(),\n",
    "#     transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),  # Normalize using ImageNet mean and std\n",
    "# ])\n",
    "\n",
    "# # Load and preprocess the real-world image\n",
    "# image_path = 'real_world_image.jpg'\n",
    "# image = Image.open(image_path)\n",
    "# input_tensor = transform(image).unsqueeze(0)  # Add batch dimension"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Deeplearning\n",
    "\n",
    "**Logistic Regression ( binary label regression )**\n",
    "\n",
    "zsum = (parameters)W dot (input vector)X + w0(bias)\n",
    "\n",
    "zsum 이 크고 작음을 통해 input을 분류한다\n",
    "\n",
    "> zsum이 크다는 것? -> activation function (sigmoid etc..) 을 통해 클래스에 속할 확률을 계산한다. \n",
    "\n",
    "> sigmoid와 같은 활성홤수는 binary classes이기에 하나의 확률값(이 때 특정 확률값을 decision boundary로 설정 가능->hyperparam)만 return하면 된다. **또한 미분이 가능하다**\n",
    "\n",
    "**loss function ( y^(예측값)과 y(label)사이 distance(loss,cost)를 return) loss function은 정답예측값에 대해서는 낮은 값, 틀린예측값에 대해서는 높은 값을 return 해야 된다.**\n",
    "\n",
    "> cross entropy loss , 파라미터별 예측값(확률 p(y|x)의 negative log), 즉 -log(p(y|x))를 minimize하는 parameter를 채택해야된다.\n",
    "\n",
    "> loss function은 인자(parameters)에 따라 값이 변하는 convex(아래로 볼록)한 함수로 minimum값이 존재한다.\n",
    "\n",
    "\n",
    "**gradient descent는 back propagate(역전파)하며 층별로 gradient를 계산하는 것이다.**\n",
    "\n",
    "> stochastic gradient descent 는 전체 데이터셋에서 일부 샘플(mini batch)을 채취해 학습에 사용 (gradient의 잡음을 줄여 수렴이 빠르다), 이 때 batch의 gradient 평균을 gradient로 사용\n",
    "\n",
    "> gradient는 parameter 차원에 따른 벡터로 계산되어지고 벡터 차원별로 parameter가 갱신된다\n",
    "\n",
    "> backpropagation후 step(갱신)단계에서 loss function의 gradient가 0인 지점인 loss의 minimum지점을 목표로 현재 인자(parameter)의 gradient 반대방향으로 인자를 진행시킨다, (parameter로부터 -gradient(기울기의 반대방향)을 더해가며 인자값을 조정한다.)\n",
    "\n",
    ">진행률은 learning rate라는 hyper parameter를 gradient에 곱해주어 정한다.\n",
    "\n",
    "**hyperparameter는 학습되어지는 것이 아니라 디자이너가 고르는 것이다.**\n",
    "\n",
    "> 은닉층 노드 개수(hidden layer input channel) 또한 hyperparameter, learning rate, epoch, threshold, loss function 종류, optimzier 종류 등등\n",
    "\n",
    "**step(갱신) 단계에서 Regularization과 dropout으로 overfit을 피하자**\n",
    "\n",
    "> overfit된 가중치 -> 큰 가중치를 penalizes하는 @*R(^)를 빼준다, L2 R-> the sum of the square of the weights(Euclidean dist) L1 R -> the sum of the absolute value of the weight (Manhattan dist)\n",
    "\n",
    "**multinomial Logistic Regression**\n",
    "\n",
    "> y^의 부류별 합은 1 -> z = [z1,z2,z3] ,이 때 activation function으로 softmax를 취해주면 softmax(z)는 각 부류별 비율로 벡터를 정규화 시켜주고 max값을 예측값으로 취한다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolution Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
